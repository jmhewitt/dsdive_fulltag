---
title: "CEE avoidance dive predictions and validation"
author: Josh
date: 18 March 2021
output: 
  pdf_document: 
    toc: yes
header-includes:
  - \usepackage{booktabs}
params:
    tgt_dir: nim_fit
    tgt_dir_val: nim_fit_val
---

# Overview

We use an extended version of the discrete space model for deep dives to assess the impact of sonar on beaked whales.  Our preliminary results support the hypothesis that sonar can cause whales to initiate deep dives as an avoidance response.  We *very* briefly describe the extended model, evidence in support of the deep dive avoidance hypothesis, and outline how we can use the extended model to study other behavioral responses.  We'd like to get your feedback and discuss your questions about the analytic approach, results, and interpretations.


# Brief model description

We have extended the discrete space model for deep dives so that it can model sequences of deep dives, shallow dives, and prolonged periods of surface observations that occur more frequently at night.  Movement through the water column is modeled via a continuous time, discrete space Markov process parameterized with respect to speed and directional preference parameters.  The parameters vary over time, with respect to a Markov model for ascent and descent stages within deep and shallow dives.  The Markov model also includes an additional surface-behavior stage used for prolonged periods of surface observations and a foraging stage for deep dives.  Given data, the stages are partially observed.  For example, the data clearly identifies deep vs. shallow dives.

The extended model requires several technical modifications for identifiability and computational tractability.  The stage model is implemented in discrete time, which implies animals can only change behavior at fixed times.  We allow this to happen at 1 minute intervals.  We also linearly interpolate the 5-minute depth bin observations to a sequence of 1-minute depth bin observations.  The interpolation lets us align the stage model with the movement model, which provides large computational savings.  The likelihood for the continuous time movement model relies on matrix exponential operators to assign probability to observed movement, which are computationally expensive.  Each matrix exponential models movement within a single dive stage, so restricting stage transition times to regular intervals lets us reuse matrix exponentials within MCMC sampling.  We further reduce the number of matrix exponentials required for inference by finely discretizing the support for the speed and directional preference parameters, which lets us pre-compute all matrix exponentials required for inference.  The fine discretization does not preclude our ability to use link functions to let covariates influence movement parameters.

The model parameters and sequence of dive stages are estimated via MCMC methods.  Hidden Markov model forward/backward algorithms let us sample stages at each iteration from their full conditional posterior distribution.  All other parameters are updated using conjugate distributions or random walk Metropolis Hastings updates, as appropriate.  We use all available data before the start of the CEEs to fit our model to data.


# Analytic framework

## Predicting departures from baseline behavior

The movement model does not include parameters for non-baseline behaviors.  So, we propose using the model's posterior and posterior predictive distributions to assess impacts of naval sonar on beaked whale diving behaviors.  The posterior distribution lets us infer whether an animal initiated an avoidance dive, and the posterior predictive distribution lets us study other behavioral responses.

Many CEEs begin when an animal is in shallow water (i.e., less than 800 m), and the coarse resolution of satellite tag data makes it difficult to determine whether the dive was going to be deep or  shallow in the absence of exposure.  The posterior distribution for dive stages lets us draw inference on the type of dive the animal was likely beginning because the baseline data do not provide a priori knowledge of what these dives were going to be.  

By comparison, the posterior predictive distribution lets us predict dive behavior after the point of exposure, presuming the animals continue to exhibit baseline behaviors.  Response behaviors are assessed by comparing predicted and observed behaviors.  Essentially, we plan to use the posterior predictive distribution as a model-based analog of the pre/post Monte Carlo tests for which we have a manuscript in review.  For example, for animals exposed to sonar at depth, the posterior predictive distribution can let us study whether the observed length of the exposed dive was longer than expected, or if its ascent stage was slower than expected.  The distribution can also potentially yield similar insights about the sequence of post-exposure dives.  A limiting factor of the posterior predictive distribution is that at some point in time, the post-exposure dives will be statistically independent from the pre-exposure dives.  Without links to strong covariates, the posterior predictive distribution will likely only have power to detect departures from baseline behavior (or returns to it) at times where post-exposure dives are still statistically dependent on the final pre-exposure behaviors.


## Validating posterior predictions

We propose using out-of-sample model validation to support the types of evidence for response behaviors discussed above.  We use the first half of the pre-exposure dive data to train the model, and the second half of the pre-exposure dive data as hold-out observations for validation.  The training and testing sets are not randomly sampled in order to ensure the model is trained using sequential observations of diving behavior.

The ability to infer whether sonar caused animals to initiate avoidance dives is validated by assessing the posterior distribution's false omission rate for deep dives.  That is, our validation goal is to estimate the probability that an arbitrary dive is actually a deep dive when it was predicted to not be a deep dive based on that dive's initial observations.  We estimate the probability in two stages.  First, we use composition sampling with the withheld dives to approximate the posterior predictive distribution for the withheld dive's type when the first 5, 10, 15, 20, 25, or 30 minutes of dive data is observed.  In all cases, we only use withheld dives that are not yet observed to be deep, otherwise the given data would completely identify the dive type.  Second, we apply thresholds to the posterior probabilities to predict the class of each validation dive (deep or not deep).  For example, we may classify dives with deep-dive probability mass less than .1 to be not deep---we say .1 is the threshold for the classification rule.  The resulting classification table of true positives, false negatives, etc. lets us approximate the false omission rate via
\begin{align}
\begin{split}
\label{eq:false_omission_rate}
  P(\text{Deep dive} \vert \text{Predicted not deep}, n \text{ obs.}) &=
    \frac{
      P(\text{Deep dive} , \text{Predicted not deep}, n \text{ obs.})
    }{
      P(\text{Predicted not deep}, n \text{ obs.})
    } \\ &\approx
    \frac{
     \#\{\text{False negatives}\}
    }{
     \#\{\text{False negatives}\} + \#\{\text{True negatives}\}
    }.
\end{split}
\end{align}

The model's ability to assess other types of response behaviors can be validated by other out of sample checks.  We have not worked on this yet.


# Results

## Validation

Table \ref{tab:n_val_dives} presents the number of validation dives available for each prediction horizon studied.  The validation data contain many deep and shallow dives for which a deep depth is not observed in the first 5 or 10 minutes of each dive.  However, the validation data contain few deep dives in which a deep depth is not attained by 15, 20, 25, or 30 minutes.  So, the validation data are less well able to estimate the false omission rate for dives in which a deep depth has not been observed by 15 minutes  \eqref{eq:false_omission_rate}.

Figure \ref{fig:validation_distributions} illustrates general characteristics of the posterior distribution for a dive's type when only the first 5--30 minutes of observations are available.  In all cases, the posterior distribution tends to assign deep dives higher probability of being deep based on the dive's initial observations.  The posterior distribution concentrates more mass near 0 and 1 as more observations from each dive is analyzed.  As desired, the mass concentrates near 0 for non deep dives, and 1 for deep dives.  However, perhaps as anticipated, the concentration is less well defined when fewer observations are analyzed from each dive.

Figure \ref{fig:classification_accuracy} compares the false omission rate \eqref{eq:false_omission_rate} as a function of the number of observations analyzed and the classification rule's threshold.  In all cases studied, the false omission rate remains low.  The false omission rate quickly decreases as more observations are analyzed from each dive, and the rate more slowly decreases as the classification threshold is lowered (i.e., as the classification rule becomes less likely to predict dives are deep).


## Avoidance dive response

Animals Zc093, Zc095, Zc096, and Zc097 were all exposed to sonar in shallow water, and each animal was subsequently observed to complete a deep dive.  No other animals we have data for were exposed in shallow water.  Table \ref{tab:final_stage} shows the posterior distribution of the final pre-exposure observation's stage for each animal.  In each case, there is low (Zc093) or very low (Zc095--097) posterior probability that the animals were beginning a deep dive.  Following the validated classification rules described above, the posterior distribution for baseline behavior classifies the initial portion of each of these dives as being part of a not deep dive.  Since the complete data record shows that each dive was, in fact, a deep dive, we conclude that the animals likely initiated a deep dive in response to sonar exposure.



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, cache = TRUE, 
                      dev = 'png', dpi = 300)
```

```{r paths}
paths = list(
  # location of mcmc output files
  path_out = file.path('output', 'mcmc', params$tgt_dir),
  path_out_val = file.path('output', 'mcmc', params$tgt_dir_val),
  # identifying characteristics of key output files
  active_samplers = '*active_samplers.rds',
  stage_sample_pattern = 'stage_samples_[0-9]+',
  param_sample_pattern = 'parameter_samples_[0-9]+',
  param_sample_column_labels = '*parameter_samples_column_labels.rds',
  validation_output = 'validate_dive_preds'
)
```

```{r load_validation_output}
library(dplyr)

validation_output_files = dir(
  path = paths$path_out_val, pattern = paths$validation_output, full.names = TRUE
)

# load output and remove entries without complete information
validation_output = do.call(rbind, lapply(validation_output_files, function(f) {
  r = readRDS(f)
  non_null = !sapply(r[,'tag'], is.null)
  data.frame(n_timepoints = as.numeric(r[non_null, 'n_timepoints']),
             prob_deep = as.numeric(r[non_null, 'prob_deep']),
             true_deep = as.logical(r[non_null, 'true_deep']),
             tag = r[non_null,'tag'])
})) %>% filter(is.finite(prob_deep))

rm(validation_output_files)
```

```{r calibration_curves, eval = FALSE}
library(ggplot2)
library(ggthemes)
library(dplyr)

df = validation_output %>% 
  mutate(prob_group = cut(prob_deep, 
                          breaks = seq(from = 0, to = 1, by = .1))) %>% 
  group_by(prob_group, n_timepoints) %>% 
  filter(is.finite(prob_group)) %>% 
  summarise(proportion_true = mean(true_deep),
            prob_group_plottable = .05 + .1 * (which.min(
              abs(seq(from = 0.05, to = .95, by = .1) - mean(prob_deep))) - 1
              )
            )

ggplot(df, aes(x = prob_group_plottable, y = proportion_true)) + 
  geom_abline(slope = 1, intercept = 0, lty = 3) + 
  geom_point() + 
  facet_wrap(~n_timepoints) + 
  theme_few() + 
  theme(panel.border = element_blank())
```

```{r validation_design}
# # distribution of deep/shallow validation dives by prediction horizon and tag
# table(deep = validation_output$true_deep,
#       tag = validation_output$tag,
#       n_timepoints = validation_output$n_timepoints)

# distribution of deep/shallow validation dives by prediction horizon
knitr::kable(
  table("Deep dive" = validation_output$true_deep,
      "Pre-Exposure Observations" = validation_output$n_timepoints), 
  caption = '\\label{tab:n_val_dives} Number of deep (TRUE row) and non-deep (FALSE row) validation dives used to assess the posterior distribution for dive stage when the first 5, 10, 15, ..., 30 1-minute observations are analyzed from each dive.'
)

# # total number of validation dives by prediction horizon
# table(validation_output$n_timepoints)
```

```{r validation_distributions, fig.cap='\\label{fig:validation_distributions} As more data is used to predict deep dives, the separation between the predictive distributions for the deep and non deep dives increases.'}
library(ggplot2)
library(ggthemes)

ggplot(validation_output, aes(x = true_deep, y = prob_deep)) + 
  geom_boxplot() + 
  ylab('P(Deep | Data)') + 
  xlab('Validation dive is deep') + 
  ggtitle('Predictive distribution by number of prediction timepoints') + 
  facet_wrap(~n_timepoints) + 
  theme_few()
```

```{r classification_accuracy, fig.cap='\\label{fig:classification_accuracy} Error rate for non-deep predictions based on classification threshold and number of observations used to make prediction.'}
library(ggplot2)
library(ggthemes)
library(dplyr)

classification_accuracies = function(n_pred_obs, cutoff) {
  
  df = validation_output %>% 
    mutate(pred_deep = prob_deep > cutoff) %>% 
    filter(n_timepoints == n_pred_obs)

  prediction.table = table(pred_deep = df$pred_deep, true_deep = df$true_deep)

  true_pos = prediction.table[2,2]
  false_neg = prediction.table[1,2]
  true_neg = prediction.table[1,1]
  false_pos = prediction.table[2,1]
    
  c(sensitivity = true_pos / (true_pos + false_neg),
    specificity = true_neg / (true_neg + false_pos),
    neg_predictive_value = true_neg / (true_neg + false_neg),
    false_omission_rate = false_neg / (true_neg + false_neg))
}

classification_configs = expand.grid(
  n_pred_obs = unique(validation_output$n_timepoints),
  cutoff = c(.05, .1, .2, .3, .4)
)


df = do.call(rbind, apply(classification_configs, 1, function(r) {
  assessment = classification_accuracies(
    n_pred_obs = r['n_pred_obs'], cutoff = r['cutoff']
  )
  data.frame(t(r), t(assessment))
}))

ggplot(df, aes(x = n_pred_obs, y = false_omission_rate, col = factor(cutoff))) + 
  geom_point() + 
  geom_line() + 
  scale_color_brewer('Classification threshold', 
                     type = 'seq', palette = 'BuPu') + 
  scale_x_continuous(breaks = seq(from = 0, to = 30, by = 5)) + 
  xlab('Prediction timepoints') + 
  ylab('P(Deep dive | Predicted not deep)') + 
  ylim(0,.12)+
  theme_few() + 
  theme(panel.border = element_blank())
```

```{r mcmc_input}
nim_pkg = readRDS(dir(path = paths$path_out, pattern = 'pkg', 
                      full.names = TRUE))
```

```{r parameter_groups}
# load targets of active samplers
active_samplers = unlist(readRDS(
  dir(path = paths$path_out, pattern = paths$active_samplers, full.names = TRUE)
))

# identify common parameter names, excluding stages
parameter_families = setdiff(
  unique(gsub(pattern = '\\[.*\\]', replacement = '', x = active_samplers)),
  'stages'
)

# associate actively sampled targets with the common parameter names
sampler_groups = lapply(parameter_families, function(pfam) {
  grep(pattern = paste(pfam, '\\[', sep = ''), 
       x = active_samplers, value = TRUE)
})
names(sampler_groups) = parameter_families
sampler_groups = sampler_groups[order(names(sampler_groups))]

rm(active_samplers, parameter_families)
```

```{r output_files}
param_sample_files = dir(path = paths$path_out, 
                         pattern = paths$param_sample_pattern,
                         full.names = TRUE)

param_samples = do.call(rbind, lapply(param_sample_files, function(f) {
  readRDS(f)
}))

colnames(param_samples) = readRDS(
  dir(path = paths$path_out, pattern = paths$param_sample_column_labels, 
      full.names = TRUE)
)

rm(param_sample_files)
```

```{r stage_files}
# identify stage sample files
stage_sample_files = dir(path = paths$path_out,
                         pattern = paths$stage_sample_pattern,
                         full.names = TRUE)

# aggregate posterior stage samples
stage_summary = matrix(as.integer(0), 
                       nrow = nim_pkg$consts$n_stages, 
                       ncol = nim_pkg$consts$n_timepoints)
for(f in stage_sample_files) {
  stage_samples = readRDS(f)
  stage_summary = stage_summary + apply(stage_samples, 2, function(samples) {
    tabulate(bin = samples, nbins = nim_pkg$consts$n_stages)
  })
}
rownames(stage_summary) = names(nim_pkg$consts$movement_types)

# normalize posteriors
stage_summary = stage_summary / colSums(stage_summary)[1]

rm(stage_samples)
```

```{r set_burn}
burn = 1:1000
```



```{r final_stage}
post_stages = do.call(rbind, lapply(1:nim_pkg$consts$n_subjects, 
                                    function(subj_ind) {
  # identify all segments associated with subject
  subj_segs = which(nim_pkg$consts$segments[,'subject_id'] == subj_ind)
  # find last index of last segment of subject
  last_ind = nim_pkg$consts$segments[tail(subj_segs, 1), 'end_ind']
  # posterior distribution of final segment of subject
  data.frame(subject = nim_pkg$consts$subject_id_labels[subj_ind],
             t(stage_summary[,last_ind]))
}))

knitr::kable(post_stages, digits = 2, 
             caption = '\\label{tab:final_stage} Posterior distribution for stage of final observation', 
             booktabs = TRUE)
```


